# Slurm MCP Server

ğŸ“– **For complete usage, installation, and API documentation, see: [documentation/MCP_SERVER_GUIDE.md](documentation/MCP_SERVER_GUIDE.md)**  
ğŸ“– **For native Slurm installation, see: [slurm_installation/SLURM_INSTALLATION_GUIDE.md](slurm_installation/SLURM_INSTALLATION_GUIDE.md)**

A comprehensive Model Context Protocol (MCP) server implementation for submitting and managing Slurm jobs. This server provides a standardized interface for interacting with Slurm workload manager through the MCP protocol, enabling seamless integration with AI assistants and other MCP clients.

## Quick Start

```bash
# Clone and setup
cd slurm-mcp
uv sync

# Start the MCP server
./server_manager.sh start

# Test functionality  
python comprehensive_capability_test.py

# Stop the server
./server_manager.sh stop
```

## Features

- **ğŸš€ Job Submission**: Submit Slurm jobs with specified core counts and resource requirements
- **ğŸ“‹ Job Management**: List, monitor, cancel, and get detailed information about jobs
- **ğŸ–¥ï¸ Node Allocation**: Interactive node allocation using `salloc` for real-time resource allocation
- **ğŸ”§ Input Validation**: Comprehensive validation of script paths and resource requirements
- **âš¡ Fast Performance**: Optimized for high-throughput job submissions
- **ğŸ›¡ï¸ Error Handling**: Robust error handling with detailed error messages
- **ğŸ“Š Multiple Transports**: Support for both stdio and SSE (Server-Sent Events) transports
- **ğŸ§ª Comprehensive Testing**: Full test suite with unit, integration, and performance tests
- **ğŸ¯ Real Slurm Integration**: Direct integration with actual Slurm workload manager
- **ğŸ“ Organized Output**: All SLURM job outputs (.out/.err files) are automatically organized in `logs/slurm_output/`
- **ğŸ”§ Modular Architecture**: Separated capabilities for better maintainability and extensibility
- **ğŸ”„ Array Job Support**: Submit and manage Slurm array jobs with ease
- **ğŸ“Š Cluster Monitoring**: Real-time cluster and node information retrieval

## Output Organization

All SLURM job output files are automatically organized in the `logs/slurm_output/` directory:

```
logs/
â””â”€â”€ slurm_output/
    â”œâ”€â”€ slurm_1234.out       # Job stdout files
    â”œâ”€â”€ slurm_1234.err       # Job stderr files
    â”œâ”€â”€ slurm_5678_1.out     # Array job outputs (format: slurm_<array_id>_<task_id>.out)
    â””â”€â”€ slurm_5678_1.err     # Array job errors
```

### File Naming Convention
- **Single Jobs**: `slurm_<job_id>.out` and `slurm_<job_id>.err`
- **Array Jobs**: `slurm_<array_job_id>_<task_id>.out` and `slurm_<array_job_id>_<task_id>.err`


## Architecture

### High-Level Architecture

The Slurm MCP Server follows a modular, layered architecture designed for scalability, maintainability, and extensibility:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MCP Client Layer                           â”‚
â”‚            (AI Assistants, CLI Tools, Web Apps)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ MCP Protocol (JSON-RPC 2.0)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MCP Server Layer                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Transport     â”‚  â”‚   Protocol      â”‚  â”‚   Tool          â”‚ â”‚
â”‚  â”‚   (stdio/SSE)   â”‚  â”‚   Handlers      â”‚  â”‚   Registry      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Function Calls
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Capabilities Layer                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Job Submit  â”‚ â”‚ Job Monitor â”‚ â”‚ Job Control â”‚ â”‚ Cluster  â”‚  â”‚
â”‚  â”‚ job_submiss â”‚ â”‚ job_status  â”‚ â”‚ job_cancel  â”‚ â”‚ cluster_ â”‚  â”‚
â”‚  â”‚ ion.py      â”‚ â”‚ job_details â”‚ â”‚ job_listing â”‚ â”‚ info.py  â”‚  â”‚
â”‚  â”‚ array_jobs  â”‚ â”‚ job_output  â”‚ â”‚             â”‚ â”‚ node_    â”‚  â”‚
â”‚  â”‚ .py         â”‚ â”‚ .py         â”‚ â”‚             â”‚ â”‚ info.py  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            Node Allocation (node_allocation.py)        â”‚  â”‚
â”‚  â”‚    Interactive resource allocation via salloc          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ System Calls
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    System Layer                                â”‚
â”‚            SLURM Workload Manager (sbatch, squeue, etc.)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Project Structure

```
slurm-mcp/
â”œâ”€â”€ README.md                          # Project documentation
â”œâ”€â”€ pyproject.toml                     # Project configuration and dependencies
â”œâ”€â”€ uv.lock                           # Dependency lock file
â”œâ”€â”€ server_manager.sh                 # Server start/stop management script
â”œâ”€â”€ move_slurm_outputs.sh             # Utility to organize output files
â”œâ”€â”€ comprehensive_capability_test.py  # Complete functionality test
â”œâ”€â”€ comprehensive_test.sh             # SLURM test job script
â”œâ”€â”€ mcp_capabilities_demo.py          # MCP capabilities demonstration
â”œâ”€â”€ test_real_functionality.py       # Real SLURM integration tests
â”‚
â”œâ”€â”€ src/                              # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ server.py                     # Main MCP server implementation
â”‚   â”œâ”€â”€ mcp_handlers.py              # MCP protocol handlers
â”‚   â””â”€â”€ capabilities/                 # Modular SLURM capabilities
â”‚       â”œâ”€â”€ __init__.py              # Capability exports
â”‚       â”œâ”€â”€ utils.py                 # Common utility functions
â”‚       â”œâ”€â”€ slurm_handler.py         # Backward compatibility module
â”‚       â”œâ”€â”€ job_submission.py        # Job submission functionality
â”‚       â”œâ”€â”€ job_status.py            # Job status checking
â”‚       â”œâ”€â”€ job_details.py           # Detailed job information
â”‚       â”œâ”€â”€ job_output.py            # Job output retrieval
â”‚       â”œâ”€â”€ job_listing.py           # Job queue listing
â”‚       â”œâ”€â”€ job_cancellation.py      # Job cancellation
â”‚       â”œâ”€â”€ cluster_info.py          # Cluster information
â”‚       â”œâ”€â”€ queue_info.py            # Queue monitoring
â”‚       â”œâ”€â”€ node_info.py             # Node information
â”‚       â”œâ”€â”€ node_allocation.py       # Interactive node allocation (salloc)
â”‚       â””â”€â”€ array_jobs.py            # Array job submission
â”‚
â”œâ”€â”€ tests/                            # Comprehensive test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                  # Test configuration and fixtures
â”‚   â”œâ”€â”€ test_capabilities.py         # Unit tests for SLURM capabilities
â”‚   â”œâ”€â”€ test_mcp_handlers.py         # Unit tests for MCP handlers
â”‚   â”œâ”€â”€ test_server_tools.py         # Tests for server async tools
â”‚   â”œâ”€â”€ test_integration.py          # End-to-end integration tests
â”‚   â”œâ”€â”€ test_node_allocation.py      # Tests for node allocation functionality
â”‚   â””â”€â”€ test_performance.py          # Performance and load tests
â”‚
â”œâ”€â”€ logs/                             # Organized output directory
â”‚   â””â”€â”€ slurm_output/                # SLURM job outputs (.out/.err files)
â”‚       â”œâ”€â”€ slurm_<job_id>.out       # Single job stdout
â”‚       â”œâ”€â”€ slurm_<job_id>.err       # Single job stderr
â”‚       â”œâ”€â”€ slurm_<array_id>_<task>.out  # Array job outputs
â”‚       â””â”€â”€ slurm_<array_id>_<task>.err  # Array job errors
â”‚
â”œâ”€â”€ documentation/                    # Additional documentation
â”‚   â””â”€â”€ MCP_SERVER_GUIDE.md          # Complete usage guide
â”‚
â”œâ”€â”€ slurm_installation/               # SLURM installation utilities
â”‚   â”œâ”€â”€ SLURM_INSTALLATION_GUIDE.md  # Installation instructions
â”‚   â”œâ”€â”€ install_slurm.sh             # Automated installation script
â”‚   â”œâ”€â”€ parallel_job.sh              # Example parallel job
â”‚   â”œâ”€â”€ quick_native_test.sh         # Quick SLURM test
â”‚   â””â”€â”€ final_verification.sh        # Installation verification
â”‚
â””â”€â”€ slurm_mcp.egg-info/              # Package metadata
```

### Modular Capabilities Design

The capabilities are organized into focused, single-responsibility modules:

#### Core Job Management
- **`job_submission.py`**: Handles job submission with full parameter support
- **`job_status.py`**: Provides real-time job status checking
- **`job_details.py`**: Retrieves comprehensive job information
- **`job_output.py`**: Manages job output file access and retrieval
- **`job_listing.py`**: Lists and filters jobs in the queue
- **`job_cancellation.py`**: Handles job cancellation and termination

#### Advanced Features
- **`array_jobs.py`**: Specialized support for SLURM array jobs
- **`cluster_info.py`**: Provides cluster-wide information and status
- **`queue_info.py`**: Monitors partition and queue states
- **`node_info.py`**: Retrieves node status and resource information
- **`node_allocation.py`**: Interactive node allocation using `salloc` for real-time resource management

#### Utilities and Compatibility
- **`utils.py`**: Common functions and utilities
- **`slurm_handler.py`**: Backward compatibility wrapper

## Prerequisites

- Python 3.10 or higher
- [uv](https://docs.astral.sh/uv/) package manager
- Linux/macOS environment (for optimal compatibility)

## Installation

### Quick Setup

```bash
# Clone and navigate to the project
cd slurm-mcp

# Install dependencies using uv
uv sync

# Install development dependencies (if not already installed)
uv add pytest pytest-asyncio --dev
```

### Manual Setup

```bash
# Initialize uv environment
uv init slurm-mcp
cd slurm-mcp

# Add production dependencies
uv add "mcp[cli]"
uv add python-dotenv

# Add development dependencies
uv add pytest pytest-asyncio --dev
```

## Usage

### 1. Running the MCP Server

#### Stdio Transport (Default)
```bash
# Start server with stdio transport
uv run python src/server.py
```

#### SSE Transport (for web clients)
```bash
# Set environment variables for SSE transport
export MCP_TRANSPORT=sse
export MCP_SSE_HOST=0.0.0.0
export MCP_SSE_PORT=8000

# Start server with SSE transport
uv run python src/server.py
```

### 2. Testing the Server

#### Run All Tests
```bash
# Run the complete test suite
uv run pytest tests/ -v

# Run with coverage
uv run pytest tests/ -v --cov=src
```

#### Run Integration Tests
```bash
# Run end-to-end integration tests
python tests/test_integration.py
```

#### Test Individual Components
```bash
# Test Slurm capabilities
uv run pytest tests/test_capabilities.py -v

# Test MCP handlers
uv run pytest tests/test_mcp_handlers.py -v

# Test server tools
uv run pytest tests/test_server_tools.py -v

# Test node allocation functionality
uv run pytest tests/test_node_allocation.py -v
```

### 3. Interactive Testing

```bash
# Start server with MCP inspector (for development)
uv run mcp dev src/server.py

# Direct JSON-RPC testing
cd slurm-mcp
echo '{"jsonrpc": "2.0", "id": 1, "method": "initialize", "params": {"protocolVersion": "2024-11-05", "capabilities": {}}}' | uv run python src/server.py
```

## API Reference

### Available Tools

The Slurm MCP Server provides a comprehensive set of tools for managing SLURM jobs and cluster resources:

#### Job Management Tools

#### `submit_slurm_job`
Submits a Slurm job script with specified resource requirements.

**Parameters:**
- `script_path` (string, required): Path to the job script file
- `cores` (integer, required): Number of CPU cores to request (must be > 0)
- `memory` (string, optional): Memory requirement (e.g., "4G", "2048M")
- `time_limit` (string, optional): Time limit (e.g., "1:00:00")
- `job_name` (string, optional): Name for the job
- `partition` (string, optional): Slurm partition to use

**Returns:**
```json
{
  "job_id": "1234",
  "status": "submitted",
  "script_path": "my_job.sh",
  "cores": 4,
  "memory": "4G",
  "time_limit": "1:00:00",
  "job_name": "my_job",
  "partition": "compute",
  "message": "Job 1234 submitted successfully"
}
```

#### `check_job_status`
Check the status of a specific job.

**Parameters:**
- `job_id` (string, required): The Slurm job ID to check

**Returns:**
```json
{
  "job_id": "1234",
  "status": "RUNNING",
  "real_slurm": true
}
```

#### `get_job_details`
Get comprehensive details about a specific job.

**Parameters:**
- `job_id` (string, required): The Slurm job ID

**Returns:**
```json
{
  "job_id": "1234",
  "details": {
    "jobname": "my_job",
    "jobstate": "RUNNING",
    "partition": "compute",
    "runtime": "00:05:30",
    "timelimit": "01:00:00",
    "numnodes": "1",
    "numcpus": "4"
  },
  "real_slurm": true
}
```

#### `get_job_output`
Retrieve job output files (stdout/stderr).

**Parameters:**
- `job_id` (string, required): The Slurm job ID
- `output_type` (string, optional): Type of output ("stdout" or "stderr", default: "stdout")

**Returns:**
```json
{
  "job_id": "1234",
  "output_type": "stdout",
  "file_path": "logs/slurm_output/slurm_1234.out",
  "content": "Job output content...",
  "real_slurm": true
}
```

#### `cancel_slurm_job`
Cancel a running or pending job.

**Parameters:**
- `job_id` (string, required): The Slurm job ID to cancel

**Returns:**
```json
{
  "job_id": "1234",
  "status": "cancelled",
  "message": "Job 1234 cancelled successfully",
  "real_slurm": true
}
```

#### `list_slurm_jobs`
List jobs in the queue with optional filtering.

**Parameters:**
- `user_filter` (string, optional): Filter by username
- `state_filter` (string, optional): Filter by job state (e.g., "RUNNING", "PENDING")

**Returns:**
```json
{
  "jobs": [
    {
      "job_id": "1234",
      "state": "RUNNING",
      "name": "my_job",
      "user": "username",
      "time": "00:05:30",
      "nodes": "1",
      "cpus": "4"
    }
  ],
  "count": 1,
  "real_slurm": true
}
```

#### Cluster Information Tools

#### `get_slurm_info`
Get cluster information and status.

**Returns:**
```json
{
  "cluster_name": "my-cluster",
  "partitions": [
    {
      "partition": "compute",
      "avail_idle": "5/10",
      "timelimit": "infinite",
      "nodes": "10",
      "state": "up"
    }
  ],
  "real_slurm": true,
  "version": "slurm-wlm 23.11.4"
}
```

#### `get_queue_info`
Get queue and partition information.

**Parameters:**
- `partition` (string, optional): Specific partition to query

**Returns:**
```json
{
  "partitions": [
    {
      "partition": "compute",
      "state": "up",
      "nodes": "10",
      "cpus": "320",
      "memory": "1280GB"
    }
  ],
  "real_slurm": true
}
```

#### `get_node_info`
Get detailed node information.

**Parameters:**
- `node_name` (string, optional): Specific node to query

**Returns:**
```json
{
  "nodes": [
    {
      "nodename": "node001",
      "state": "idle",
      "cpus": "32",
      "memory": "128GB",
      "features": "gpu,nvme"
    }
  ],
  "real_slurm": true
}
```

#### Advanced Features

#### `submit_array_job`
Submit a Slurm array job.

**Parameters:**
- `script_path` (string, required): Path to the job script
- `array_range` (string, required): Array range (e.g., "1-10", "1-100:2")
- `cores` (integer, optional): Cores per array task (default: 1)
- `memory` (string, optional): Memory per array task
- `time_limit` (string, optional): Time limit per array task
- `job_name` (string, optional): Base name for the array job
- `partition` (string, optional): Slurm partition to use

**Returns:**
```json
{
  "array_job_id": "1234",
  "array_range": "1-10",
  "status": "submitted",
  "total_tasks": 10,
  "cores_per_task": 2,
  "message": "Array job 1234 with 10 tasks submitted successfully",
  "real_slurm": true
}
```

### Node Allocation Tools

The Slurm MCP server provides interactive node allocation capabilities using `salloc` for real-time resource management. This allows you to allocate compute nodes for interactive work sessions.

#### `allocate_nodes`
Allocate compute nodes using `salloc` for interactive sessions.

**Parameters:**
- `nodes` (integer, optional): Number of nodes to allocate (default: 1)
- `cores` (integer, optional): Number of cores per node (default: 1)
- `memory` (string, optional): Memory requirement (e.g., "4G", "2048M")
- `time_limit` (string, optional): Time limit (e.g., "1:00:00", default: "01:00:00")
- `partition` (string, optional): Slurm partition to use
- `job_name` (string, optional): Name for the allocation (default: "mcp_allocation")
- `immediate` (boolean, optional): Whether to return immediately without waiting (default: false)

**Returns (Success):**
```json
{
  "allocation_id": "5817",
  "status": "allocated",
  "nodes": 1,
  "cores": 1,
  "memory": "4G",
  "time_limit": "01:00:00",
  "partition": "compute",
  "job_name": "mcp_allocation",
  "allocated_nodes": ["node001"],
  "nodelist": "node001",
  "real_slurm": true
}
```

**Returns (Timeout):**
```json
{
  "error": "Allocation request timed out (10 seconds)",
  "status": "timeout",
  "real_slurm": true,
  "message": "The allocation request took too long. Resources may not be immediately available.",
  "timeout_duration": 10,
  "suggestion": "Try with immediate=True for quicker response or check resource availability"
}
```

**Returns (Resources Unavailable):**
```json
{
  "error": "No resources available for allocation",
  "status": "failed",
  "real_slurm": true,
  "message": "Unable to allocate resources",
  "reason": "resources_unavailable"
}
```

#### `get_allocation_status`
Check the status of a node allocation.

**Parameters:**
- `allocation_id` (string, required): The allocation ID to check

**Returns:**
```json
{
  "allocation_id": "5817",
  "status": "active",
  "state": "RUNNING",
  "time_used": "0:15:30",
  "nodes": ["node001"],
  "real_slurm": true
}
```

#### `deallocate_nodes`
Deallocate nodes by canceling the allocation.

**Parameters:**
- `allocation_id` (string, required): The allocation ID to cancel

**Returns:**
```json
{
  "allocation_id": "5817",
  "status": "deallocated",
  "message": "Allocation 5817 deallocated successfully",
  "real_slurm": true
}
```

### Node Allocation Usage Examples

#### Basic Node Allocation
```python
# Allocate a single node with 2 cores for 30 minutes
result = allocate_nodes(
    nodes=1,
    cores=2,
    memory="4G",
    time_limit="00:30:00",
    partition="compute"
)

if result["status"] == "allocated":
    allocation_id = result["allocation_id"]
    print(f"Allocated node(s): {result['allocated_nodes']}")
    
    # Use your allocation for interactive work...
    
    # Clean up when done
    deallocate_nodes(allocation_id)
```

#### Quick Allocation with Timeout
```python
# Quick allocation with immediate return if resources busy
result = allocate_nodes(
    nodes=1,
    cores=1,
    time_limit="00:15:00",
    immediate=True  # Return quickly if resources not available
)

if result["status"] == "timeout":
    print("Resources busy, try again later")
elif result["status"] == "allocated":
    print(f"Got allocation: {result['allocation_id']}")
```

#### Allocation Status Monitoring
```python
# Check allocation status
status = get_allocation_status("5817")
print(f"Allocation state: {status['state']}")
print(f"Time used: {status['time_used']}")
print(f"Nodes: {status['nodes']}")
```

### Error Responses

All tools return standardized error responses:

```json
{
  "content": [{"text": "{\"error\": \"Error description\"}"}],
  "_meta": {"tool": "tool_name", "error": "ErrorType"},
  "isError": true
}
```

### MCP Protocol Flow

1. **Initialize**: Client sends initialization request
2. **List Tools**: Client requests available tools
3. **Call Tool**: Client calls tool with parameters
4. **Response**: Server returns results or error

## Examples

### Basic Job Submission

Create a job script and submit it via MCP:
```bash
# Create job script
cat > my_job.sh << 'EOF'
#!/bin/bash
#SBATCH --job-name=test_job
echo "Hello from Slurm job!"
sleep 10
echo "Job completed"
EOF

# Submit via comprehensive test
python comprehensive_capability_test.py
```

### Using Python Client

```python
import json
import subprocess

def submit_job(script_path, cores):
    # MCP protocol requests
    requests = [
        {"jsonrpc": "2.0", "id": 1, "method": "initialize", 
         "params": {"protocolVersion": "2024-11-05", "capabilities": {}}},
        {"jsonrpc": "2.0", "method": "notifications/initialized"},
        {"jsonrpc": "2.0", "id": 2, "method": "tools/call",
         "params": {"name": "submit_slurm_job", 
                   "arguments": {"script_path": script_path, "cores": cores}}}
    ]
    
    # Submit to MCP server
    input_str = '\n'.join(json.dumps(req) for req in requests) + '\n'
    result = subprocess.run(['uv', 'run', 'python', 'src/server.py'],
                          input=input_str, capture_output=True, text=True)
    
    # Parse response
    responses = [json.loads(line) for line in result.stdout.strip().split('\n') if line.strip()]
    return responses[-1]

# Usage
response = submit_job("my_job.sh", 4)
print(f"Job submitted: {response}")
```

## Configuration

### Environment Variables

- `MCP_TRANSPORT`: Transport type (`stdio` or `sse`, default: `stdio`)
- `MCP_SSE_HOST`: Host for SSE transport (default: `0.0.0.0`)
- `MCP_SSE_PORT`: Port for SSE transport (default: `8000`)

### Configuration File

Create a `.env` file in the project root:
```env
MCP_TRANSPORT=stdio
MCP_SSE_HOST=localhost
MCP_SSE_PORT=8000
```

## Testing

### Quick Testing
```bash
# Run comprehensive capability test (includes all features)
python comprehensive_capability_test.py

# Run node allocation comprehensive test
python test_node_allocation_comprehensive.py

# Run all tests
uv run pytest tests/ -v

# Run specific test categories
uv run pytest tests/test_capabilities.py -v
uv run pytest tests/test_mcp_handlers.py -v
uv run pytest tests/test_node_allocation.py -v
```

### Node Allocation Testing

Test the node allocation functionality specifically:

```bash
# Test node allocation with clean environment
scancel --user=$USER  # Clean up any existing jobs
python test_node_allocation_comprehensive.py

# Test basic allocation manually
python -c "
import sys; sys.path.insert(0, 'src')
from capabilities.node_allocation import allocate_nodes, deallocate_nodes
result = allocate_nodes(nodes=1, cores=1, time_limit='00:05:00', immediate=True)
print('Result:', result)
if result.get('allocation_id'):
    deallocate_nodes(result['allocation_id'])
"
```

## Development

### Adding New Features

1. **Add Capability**: Implement core logic in `src/capabilities/`
2. **Add Handler**: Create MCP wrapper in `src/mcp_handlers.py`
3. **Add Tool**: Register tool in `src/server.py`
4. **Add Tests**: Create comprehensive tests in `tests/`

### Development Workflow

```bash
# Setup development environment
git clone <repository>
cd slurm-mcp
uv sync

# Make changes and test
uv run pytest tests/ -v
python comprehensive_capability_test.py
```

## Troubleshooting

### Common Issues

1. **Import Errors**: Ensure proper Python path
2. **MCP Protocol Errors**: Check server logs
3. **SLURM Not Available**: Install SLURM or run in mock mode

### Debug Mode

```bash
export PYTHONPATH="$(pwd)/src"
export MCP_DEBUG=1
uv run python src/server.py
```


## License

This project is licensed under the MIT License. See LICENSE file for details.

## Support

For questions and support:
- Create an issue in the repository
- Check the troubleshooting section
- Review test examples for usage patterns

